<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - cocowen&#39;s blog</title>
        <link>https://cocowenlw.github.io/posts/</link>
        <description>All Posts | cocowen&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 13 Oct 2021 19:42:48 &#43;0800</lastBuildDate><atom:link href="https://cocowenlw.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Center Selection Algorithm</title>
    <link>https://cocowenlw.github.io/first_post/</link>
    <pubDate>Wed, 13 Oct 2021 19:42:48 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://cocowenlw.github.io/first_post/</guid>
    <description><![CDATA[Center Selection Algorithm &emsp;&emsp;In the k-means algorithm, we can start with (i) using an initial partition {S1, S2, ..., Sk} or with (ii) using initial centers {c1, c2, ..., ck}. Design a good initialization method for k-means algorithm for (i) and also for (ii)   题目相当于问k-means方法的初始化。常规的来说有三种方法，第一种随机选择初始点，第二种k-means++，第三种基于层次的初始化。对于聚类初始化算法来说，其效率很大程度上是跟其初始数据有关的，所以很难说某种算法非常的优秀，但总体上来说，k-means++应该是使用最广泛的一种。
首先来讲一下kmeans++算法
1、随机选取第一个中心
2、计算每个样本和已有聚类中心的最短距离。下一个中心被选中的概率与这个最短距离的平方成正比。D(xi)=min||xi−cr||2 其中D(xi)越大，选中概率越大。
3、然后一直重复第二个步骤，直到选出K个中心点来。
然后说一下我的方法，我选择第一个中心点的时候不是随机选择，而是想选择密度大的中心点。所以我计算了每一个点到其他点距离的总和，并将总和最小的那个点，作为我的初始点。然后就是选择离已选择中心尽可能远的点，与k-mean++的思想类似，但是没有用轮盘法来进行选择。 下面是我简单举例展示出我的初始化和随机初始化的区别。]]></description>
</item></channel>
</rss>
